{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JCS Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. After executing \"Initialization\", change the parameters under \"User Options\" specific to your needs. It is important for \"output_s3_subkey\" to be unique for each brand new query.\n",
    "\n",
    " 2. Execute the \"Prepare Input\" and \"Launch Query\" cells. If there is something wrong with the input csv or selector, it will raise an error before the query takes place, and you'll get a chance to see what it was and fix it.\n",
    "\n",
    " 3. Wait for the query to finish. You cannot tell when a batch query is finished using just this notebook, although it usually finishes in around 15 minutes regardless of size.\n",
    "\n",
    " 4. After the query is finished, execute the rest of the cells, starting from \"Collect Query Results\". Unlike the query itself, the amount of time and memory it takes depends on the number of points and amount of requested data.\n",
    "\n",
    "#### Selector Info:\n",
    "\n",
    "The selector chooses what data is queried. The notebook supports CSG>=2.1. Notes:\n",
    " - `None` will return data from the final step of the JCS pipeline\n",
    " - If you want a subset, provide a *list* of values, even if the list has only one thing in it\n",
    " - You cannot ask for a subset of metrics unless there is only one peril\n",
    " - \"baseline\"/\"baseline_average\" are for year 1995\n",
    "\n",
    "#### Important\n",
    " - If you've previously performed a query and want to reload an existing raw file, then after the \"Prepare Input\" section skip to the \"Optional - load existing raw file\" section and uncomment out that code\n",
    " - This notebook is not intended to run hundreds of thousands of points. If that is what you want to do, you'll need to use separate code in conjunction with this notebook. Instructions on how to do that aren't listed here. The recommended upper limit is 50,000 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import shutil\n",
    "\n",
    "consoleHandler = logging.StreamHandler('jcs-query-notebook')\n",
    "consoleHandler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logging.getLogger().addHandler(consoleHandler)\n",
    "\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from typing import Optional, List\n",
    "from dask.distributed import Client\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from jupiter_csg_query.core import URL, Selector, Result, Location\n",
    "from jupiter_csg_query.util import generator\n",
    "from jupiter_csg_query.source import jcs\n",
    "from jupiter_csg_query.runtime.batch import BatchProvider\n",
    "from jupiter_csg_query.runtime.parallel.provider import ParallelProvider\n",
    "from jupiter_csg_query.source.jcs.query_source import PartitionJcs  # Not used?\n",
    "\n",
    "import postprocess.postprocessing_jcs as postprocessing_jcs\n",
    "import preprocess.geocoding as geocoding\n",
    "from releases.selector_jcs import jcs_selector\n",
    "from releases.selector_jcs_dem import jcs_dem_selector\n",
    "from validate.validate import validate_query_input\n",
    "from postprocess import collect, util\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current user\n",
    "stage = \"production\"\n",
    "output_s3_subkey = \"test/unique_subkey\"\n",
    "input_url = \"infile.csv\"\n",
    "output_url = \"outfile.csv\"\n",
    "\n",
    "#\n",
    "# Create a selector for JCS query\n",
    "#\n",
    "\n",
    "# Required values\n",
    "# Note: the same thresholds_version is used for all queries at present\n",
    "thresholds_version = 1\n",
    "# Defines which version of source files to use\n",
    "csg_version = \"2.2\"\n",
    "# Defines how source data is treated, e.g. treat the data as if it were\n",
    "# from the 2.2 release in terms of peril/metric availability\n",
    "metric_set = \"2.2\"\n",
    "\n",
    "# Optional arguments; set these to None to use the default value\n",
    "# Otherwise, should be specified with lists. If specific metrics are\n",
    "# desired, a single peril must be specified.\n",
    "check_sets = None  # default: [\"combined_checkset\"]\n",
    "perils = None  # default: all perils\n",
    "metrics = None  # default: [\"combined_metric\"]\n",
    "epochs = None  # default: all epochs\n",
    "scenarios = None  # None  # default: [\"combined_scenario\"]\n",
    "uq_bands = None  # uncertainty bands; default: [\"combined_uq\"]\n",
    "\n",
    "# provider_type options are \"batch\" or \"parallel\";\n",
    "# \"parallel\" should only be used if you anticipate\n",
    "# having 15 or less points\n",
    "provider_type = \"batch\"\n",
    "\n",
    "# enable this only if the input dataframe is missing lat-lons\n",
    "# and we need to geocode the addresses.\n",
    "# there are additional options in the geocode cell but you usually\n",
    "# won't need to change them.\n",
    "do_geocode = False\n",
    "\n",
    "validate_input = True\n",
    "check_for_missing_data = False  # TODO\n",
    "include_debug_messages = False\n",
    "dry_run = True\n",
    "ignore_missing_record_errors = True\n",
    "missing_record_retries = 5\n",
    "\n",
    "# currently all `region` does is, if it isn't None, create a column called \"jupiterRegion\" with all rows having the given value \n",
    "region = None\n",
    "\n",
    "# if you want to manually define or edit the selector(s) (NOT RECOMMENDED), do so here\n",
    "selector = jcs_selector(\n",
    "    metric_set=metric_set,\n",
    "    csg_ver=csg_version,\n",
    "    thresholds_version=thresholds_version,\n",
    "    perils=perils,\n",
    "    check_sets=check_sets,\n",
    "    epochs=epochs,\n",
    "    scenarios=scenarios,\n",
    "    metrics=metrics,\n",
    "    bands=uq_bands,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If querying JCS DEM results, use the following selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_dem = jcs_dem_selector(\n",
    "    dem_version=8,\n",
    "    thresholds_version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# please do not change this cell unless you know what you're doing\n",
    "# --------------------------------------------------------------------------------\n",
    "csg_bucket = \"jupiter-climatescoreglobal-eos\"\n",
    "output_bucket = \"jupiter-climatescoreglobal-eos\" # alternatively, this might be a customer-specific bucket\n",
    "source_loc = f's3://{csg_bucket}/{stage}/'\n",
    "work_prefix = f\"reports/query/{output_s3_subkey}/\"\n",
    "work_loc = f's3://{output_bucket}/{work_prefix}'\n",
    "\n",
    "# location for source, work-area and output files\n",
    "source_url = URL(source_loc)\n",
    "work_area_url = URL(work_loc)\n",
    "\n",
    "# define current wind units so we don't convert twice\n",
    "current_wind_units = 'mps'\n",
    "\n",
    "batch_settings = dict(\n",
    "    work_area_url=work_area_url,\n",
    "    parallelism=250,\n",
    "    job_queue='csg-global-prod-validation',\n",
    "    job_definition='csg-query-dev:13',\n",
    "    region='us-east-1'\n",
    ")\n",
    "\n",
    "if include_debug_messages:\n",
    "    logging.getLogger('jupiter.csg.query').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding (optional)\n",
    "You can pass in your own `address_cols` and `loc_id_col` if your dataframe doesn't match the expected default.\n",
    "\n",
    "Default `address_cols` to be geocoded are: (\"streetAddress\",\"streetAddress2\",\"cityName\",\"admin1Code\",\"postalCode\",\"countryCodeISO2A\")\n",
    "Default `loc_id_col` is `locationId`\n",
    "You can also set `geocode_log_file` to `None` if you don't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(input_url)\n",
    "input_df.columns = input_df.columns.str.strip()\n",
    "if do_geocode:\n",
    "    geocode_log_file = output_url.replace(\".csv\", \"_geocode_status.csv\")\n",
    "    input_df = geocoding.geocode_dataframe(input_df, log_file=geocode_log_file) # insert address_cols, loc_id_col params if needed\n",
    "if validate_input:\n",
    "    validate_query_input(input_df)\n",
    "input_df = input_df.sort_values(by=\"locationId\")\n",
    "print(input_df.info())\n",
    "print(f\"Provider type: {provider_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch JCS Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime query executor\n",
    "# this will block until batch is done\n",
    "util._launch_query_jcs(\n",
    "    input_df=input_df,\n",
    "    selector=selector,\n",
    "    dry_run=dry_run,\n",
    "    source_url=source_url, \n",
    "    batch_settings=batch_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch JCS DEM Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not execute this cell unless you are hitting \"Run All Cells\", all it does is wait for 25 minutes, which is a safe amount to wait for batch to finish *IF* everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shouldn't be needed anymore since the query has its own batch waiter\n",
    "\n",
    "# if provider_type == 'batch':\n",
    "#     time.sleep(60 * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the query to finish before executing this cell. You cannot tell when a batch query is finished using just this notebook, although it usually finishes in less than 15 minutes regardless of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util._launch_collect(\n",
    "    work_bucket=output_bucket,\n",
    "    work_prefix=work_prefix,\n",
    "    selector=selector,\n",
    "    csg_version=metric_set,\n",
    "    data_source=\"jcs\",\n",
    ")\n",
    "\n",
    "# TODO: Future work - collect JCS DEM query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - load existing raw file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the collect query section was previously successful and you'd like to load an existing raw file, uncomment and use this section after having executed the \"Prepare Input\" section. Do NOT uncomment out this code if you are performing a query under a unique `output_s3_subkey` for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f\"s3://{output_bucket}/{work_prefix}all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - check for missing records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are no points missing, this is fast.\n",
    "\n",
    "Otherwise, it may take some time depending on how much has to be checked. TODO is better optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Future work\n",
    "if check_for_missing_data:\n",
    "    util._check_missing(df, input_df, selector, work_area_url, output_bucket, work_loc, work_prefix, missing_record_retries, dry_run, ignore_missing_record_errors,\n",
    "                   source_url, batch_settings, csg_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = postprocessing_jcs.postprocess_query(df, input_df,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra User Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blank cell is for the user for any additional code they'd like to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving will take a good amount of time if the dataframe is big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_url, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
